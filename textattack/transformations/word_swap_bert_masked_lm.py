from collections import deque

import torch
from transformers import BertForMaskedLM, BertTokenizerFast

from textattack.shared import utils
from textattack.transformations.word_swap import WordSwap


class WordSwapBERTMaskedLM(WordSwap):
    """
    Transforms an input by replacing a word with the most likely words generated by BERT-Masked LM.
    """

    def __init__(
        self,
        max_candidates=10,
        language_model="bert-base-uncased",
        max_length=256,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.max_candidates = max_candidates
        self.lm_type = language_model
        self.max_length = max_length
        self.subword_expand_limit = 2

        self._lm_tokenizer = BertTokenizerFast.from_pretrained(
            "bert-large-uncased-whole-word-masking"
        )
        self._language_model = BertForMaskedLM.from_pretrained(
            "bert-large-uncased-whole-word-masking"
        )
        self._language_model.to(utils.device)
        self._segment_tensor = (
            torch.zeros(self.max_length, dtype=torch.long).unsqueeze(0).to(utils.device)
        )
        self._language_model.eval()

    def _call_language_model(self, ids, masked_index, k):
        """
        Query BERTMaskedLM with ids to get top-k predictions for masked position
        """
        ids_tensor = torch.tensor([ids]).to(utils.device)
        with torch.no_grad():
            preds = self._language_model(
                ids_tensor, token_type_ids=self._segment_tensor
            )[0]

        mask_token_logits = preds[0, masked_index]
        top_ids = torch.topk(mask_token_logits, self.max_candidates)[1].tolist()

        return top_ids

    '''
    def _expand_subword(self, ids, index, subword_token):
        """
        Query BERT again to fully generate word from `subword_token`
        Args:
            ids (list[int]): list of ids
            index (int): index of mask token
            subword_token (str): string sub-word token predicted for masked position
        """
        ids[index] = self._lm_tokenizer.convert_tokens_to_ids(subword_token)
        worklist = deque([(ids, index, 0)])
        final_expansions = []
        
        while worklist:
            ids, index, num_expansion = worklist.popleft()
            if num_expansion > self.subword_expand_limit or "##" not in self._lm_tokenizer.convert_ids_to_tokens(ids[index]):
                expanded_word = self._lm_tokenizer.decode(ids[index:index+num_expansion+1]).replace("##", "")
                final_expansions.append(expanded_word)
            else:
                ids.insert(self._lm_tokenizer.mask_token_id, index-1)
                ids = ids[:self.max_length]
                if self._lm_tokenizer.sep_token_id not in ids:
                    ids[-1] = self._lm_tokenizer.sep_token_id
                index -= 1
                num_expansion+=1
                top_ids = self._call_language_model(ids, index, self.max_candidates)

                for id in top_ids:
                    new_id = ids.copy()
                    new_id[index] = id
                    worklist.append((new_id, index, num_expansion))

        return final_expansions
    '''

    def _get_replacement_words(self, current_text, index):
        """
        Returns a set of replacements for word at `index` predicted by BERT model
        """
        masked_attacked_text = current_text.replace_word_at_index(
            index, self._lm_tokenizer.mask_token
        )
        ids = self._lm_tokenizer.encode(
            masked_attacked_text.text,
            max_length=self.max_length,
            pad_to_max_length=True,
        )

        masked_index = ids.index(self._lm_tokenizer.mask_token_id)
        top_ids = self._call_language_model(ids, masked_index, self.max_candidates)
        replacement_words = []
        subword_tokens = []
        for id in top_ids:
            token = self._lm_tokenizer.convert_ids_to_tokens(id)
            if check_if_word(token):
                replacement_words.append(token)
            elif "##" in token:
                subword_tokens.append(token)

        """
        for token in subword_tokens:
            expanded_tokens = self._expand_subword(ids, masked_index, token)
            print("expanded:",expanded_tokens)
            for expanded_token in expanded_tokens:
                if check_if_word(expanded_token):
                    replacement_words.append(expanded_token)
        """
        return replacement_words

    def _get_transformations(self, current_text, indices_to_modify):
        transformed_texts = []

        for i in indices_to_modify:
            replacement_words = self._get_replacement_words(current_text, i)
            print(replacement_words)
            transformed_texts_idx = []
            for r in replacement_words:
                transformed_texts_idx.append(current_text.replace_word_at_index(i, r))
            transformed_texts.extend(transformed_texts_idx)

        return transformed_texts

    def extra_repr_keys(self):
        return ["max_candidates", "lm_type", "max_length"]


def recover_word_case(word, reference_word):
    """ Makes the case of `word` like the case of `reference_word`. Supports
        lowercase, UPPERCASE, and Capitalized. """
    if reference_word.islower():
        return word.lower()
    elif reference_word.isupper() and len(reference_word) > 1:
        return word.upper()
    elif reference_word[0].isupper() and reference_word[1:].islower():
        return word.capitalize()
    else:
        # if other, just do not alter the word's case
        return word


def check_if_word(word):
    for c in word:
        if not c.isalpha():
            return False
    return True
